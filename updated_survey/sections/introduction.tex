\section{Introduction}
\revb{For years, computational and dataset limitations hindered the practical implementation of vision algorithms on non-von Neumann architectures. Advances in fabrication technology following Moore's law have alleviated computational constraints. The availability of billions of nanoscale transistors has enabled configurable heterogeneous architectures such as GPGPUs and SoC FPGAs. Furthermore, the availability of large-scale datasets has made these architectures viable for inherently parallel, bio-inspired deep learning methods}~\cite{lecun1998gradient, sze2017efficient}. \revb{Although ASIC chips provide fast and power-efficient solutions for deep learning applications, they incur costly version control due to technology evolution. Reconfigurable computing on FPGAs decouples design from Moore's law since hardware description language can be readily mapped to new technologies. FPGAs bridge the gap between purely software and purely hardware solutions for diverse applications} \cite{kilts2007advanced, kuon2008fpga, chen2006fpga, dehon1999reconfigurable}.

\revb{For vision processing, two primary approaches exist: extracting features and descriptors from input data, or training end-to-end learnable data-centric methods. Feature extraction algorithms employ mathematical operations such as gradient calculation and Fourier analysis to extract contours, edges, and optical flow. In contrast, learnable algorithms process features after training on available data. Conventional deep neural networks exemplify the latter approach, providing non-von Neumann parallel solutions for vision applications. Non-von Neumann architectures eliminate the need to sequentially map programs to instructions in memory, thereby enabling edge processing and reducing data transfer time and energy between memory and compute units. However, full-precision features and weight values in conventional deep networks are both energy-inefficient and memory-intensive. Quantization techniques applied to full-precision data and pruning of unimportant connections are two techniques that reduce computation and storage requirements. An alternative approach for reducing energy consumption and computation involves emulating biological neurons by feeding neural networks with spikes. Spiking neural networks reduce computation by evaluating digital inputs on a temporal basis when spikes arrive. This neuromorphic paradigm is based on the pioneering mathematical model by Hodgkin and Huxley derived from the squid giant axon} \cite{hodgkin1952quantitative}.


\revb{This work provides a comprehensive complement to existing vision surveys on FPGAs} \cite{venieris2018toolflows, wang2018survey, abdelouahab2018accelerating, guo2017survey, hamzah2016literature, garcia2014survey}.


\revb{This survey addresses the following objectives:}
\begin{description}
\item[$\bullet$] \revb{Comprehensive analysis of vision algorithms and their hardware implementations.}

\item[$\bullet$] \revb{Coverage of publications from top FPGA conferences and their optimization methods.}

\item[$\bullet$] \revb{Comparison of results based on device, resources, power, and throughput.}

\item[$\bullet$] \revb{Conclusions on optimal vision algorithms for energy-efficient and real-time processing.}
\end{description}
