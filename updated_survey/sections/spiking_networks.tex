\section{spiking neural networks and event-based vision}

Rods and cones in the visual system can sense stimuli of different wavelengths. They apply different weights on input stimuli and accumulate the logarithmic response to reflect chrominance (color) and luminance (brightness). Their result would feed the associative cortex for linking objects and the occipital cortex for processing patterns. These visual cells also hierarchically give different responses \cite{hubel1979brain, riesenhuber1999hierarchical}.  The simple visual cells track the general structure while the complex visual cells detect all the details. This hierarchical concept drives neuromorphic vision sensors and spiking neural networks. The dynamic vision sensor (DVS) \cite{lichtsteiner2008128} takes inspiration from the "transient" pathway, a part of the vision system which detects the dynamic visual information. This "transient" pathway occupies 30\% of our visual system  \cite{steffen2019neuromorphic}.

The event-based bio-sensors \cite{lichtsteiner2008128} trigger an output spike in a pixel location whenever the brightness changes logarithmically above a threshold value. Similarly, the neurons connected to the event-based sensor hierarchically trigger a spike whenever their state surpasses a threshold value. The very first layers of this receptive field implement the difference of Gaussian for detecting edges and corners and Gabor filter for detecting the orientation of these edges. This receptive field gets complicated in the last layers for detecting more detailed features. The neurons in SNNs do not generate output spikes if they do not receive input spikes, which means skipping computation on these neurons and save on energy consumption. On the contrary, the traditional convolutional nets do computation on all regions in fixed time intervals \cite{camunas2014event, gallego2019event}. Feeding the event-based data to the latter end neuromorphic system helps to avoid energy-inefficient conversion from regular input training patterns to the sequence of spikes \cite{wang2017energy}.

\subsection{Neuromorphic computing}
The whole concept behind neuromorphic computing is the design of energy and space-efficient system inspired by the neuroscientist's insights about the complex behavior of neocortex \cite{merollad2014million}. In terms of scale, the mammalian cortex comprises about 100 billion neurons, and each neuron connects to from 10 to 10,000 synapses. With the current silicon technology, it is impossible to have the same brain structure. However, with progress in Moore's law, we see several endeavors on designing the large-scale neuromorphic processors \cite{li2016heterogeneous} such as IBM TrueNorth \cite{akopyan2015truenorth}, Intel Loihi \cite{davies2018loihi}, Stanford Neurogrid \cite{benjamin2014neurogrid}, Heidelberg BrainScaleS \cite{schmitt2017neuromorphic}, and Manchester SpiNNaker \cite{furber2014spinnaker}. A brief review of large-scale neuromorphic computing systems is available in \cite{furber2016large}, and Figure~\ref{neurochips} provides a comparison for each of which. However, initiating a new project and design and fabrication based on new technology is quite costly based-on budget and time. The solution would be a reconfigurable hardware design on FPGAs that adapts to different technologies depending on the available resources.

\subsection{FPGAs for event-based processing}
Comparing to the CPUs and GPUs, FPGAs give higher throughput and energy efficiency for SNNs \cite{sripad2018snava}. Moreover, FPGAs give flexibility and remove the lengthy and costly process of ASIC chip design and fabrication \cite{maguire2007challenges}. With the register transfer level (RTL) codes, independent of the technology, the only design implementation restriction is the available resources in FPGAs. We can generate RTL codes for spiking nets via high-level synthesis (HLS) tools based on C++ and C \cite{pearson2005design} or direct implementation by Verilog or VHDL. Although using hardware description language (HDL code) \cite{akbarzadeh2018scalable} give low-level optimization solution for SNNs, for faster prototyping on FPGAs, HLS, OpenCL or MATLAB toolbox would be handy \cite{alfaro2019improving, alfaro2019prototyping, jin2019simulation, wu2015development}.

\textbf{FPGAs for real time simulation of SNNs for neurobiological disorders: }Parallel simulation of neurons on FPGAs help to analyze the neurobiological disorders \cite{khoyratee2019optimized, han2020hardware}. To discovering neuroprostheses, we should stimulate the neuronal cells with the desired state that is possible with a real-time neuron emulator \cite{khoyratee2019biomimetic, zbrzeski2016bio}. The parallel neuron model also helps a better understanding of how the brain works \cite{yang2018fpga}, and even they can replace the damaged neurons \cite{ambroise2013biorealistic}. The SNNs design in FPGAs can also have a parallel self-repair capability of neural nets called spiking astrocyte neural networks \cite{karim2018fpga, karim2020astrobyte, johnson2017homeostatic}.


\textbf{Routing and communication on FPGAs:} FPGA routing cannot accommodate the high level of connectivity inherent in complex SNNs
Each neuron in the brain connects to 1000-10000 synapses, which is a challenge looking at the FPGA's limited routing capability \cite{harkin2009reconfigurable}. This high level of connectivity helps to have a fault-tolerant system, and in the brain, this is called graceful degradation. Graceful degradation means all the times the neurons lose connectivity, but it is hard to sense weaken brains functioning. Network on chip (NoC) structures \cite{carrillo2012advancing, cawley2011hardware, morgan2009exploring, renzini2019quantitative, luo2018fpga, zhang2019asynchronous} help to optimize the transfer of packets of spikes between configurable logics on a shared bus in FPGAs assuming a dense level of connectivity in SNNs. The NoC rules also enhance the scalability of the spiking nets and interneuron communication. A variety of routing techniques exists in NoC, such as Manhattan-style in \cite{rubin2003design} and multi-stage switching in \cite{renzini2019quantitative}, that are among the oldest and the newest methods respectively.

\textbf{Compute efficient FPGAs: }The FPGA platforms can very cost-efficiently emulate business scale neuromorphic architectures \cite{valancius2020fpga, wang2020sies}. Compute efficient techniques such as approximate computing \cite{wang2017energy, wang2016liquid}, which neglect redundant computation, help to make SNNs viable on FPGAs. Moreover, the multiplication operation can be replaced with the CORDIC algorithm and lookup tables to make the design compute efficient \cite{heidarpur2019cordic}.  Reducing the data width in weights precision \cite{zambelli2018half} helps redundant processing of full precision implementation.  Optimization on memory hierarchy also saves on the number of memory accesses and consequent power dissipation \cite{nallathambi2020probabilistic, saha2020cynapse}.


\textbf{Learning and inference on FPGAs:} Three significant neuron models exist to implement the inference step of SNNs. The most complex model would be Hodgkin-Huxley equations \cite{alfaro2019prototyping, kousanakis2017architecture}, while the simpler versions like Izhikevich \cite{lammie2018unsupervised, choi2015implementation} and leaky integrate and fire \cite{slepova2018synthesis, han2020hardware, guo2019systolic, yi2019implementation, roshdy2020generic, wan2016efficient, mostafa2017fast} proved promising simplicity and accuracy for FPGA implementation. One solution for training SNNs is self-supervised learning by spiking-timing dependent plasticity (STDP) \cite{lammie2018unsupervised, wang2017energy, heidarpur2019cordic, kuang2019digital, xia2020real, liu2019energy}. However, this method does not compete well with the available supervised learning methods on ANNs. The solution is proceeding with the supervised backpropagation algorithms on ANN and then converting the weight values to the SNN \cite{ju2020fpga, wang2020sies}. Most of the time, acceleration happens on the SNN inference step on FPGAs while the model is trained on the software with a stream of updated weights to the hardware \cite{walravens2019spiking}.


\textbf{FPGAs for event-based sensors:} The spike outputs from event-based camera sensors can directly feed the spiking neural networks for pattern recognition applications \cite{camunas2019low}. The SNNs can also accurately process visual features \cite{sun2017spiking}. FPGAs can also give super-speed transmission of event packets and noise removal \cite{linares2015usb3, linares2019low}. The parallel structures on FPGAs speed up computation on vital algorithms such as optical flow calculation and object detection \cite{liu2017block, aung2018event, liu2019live}. Coupling FPGAs with DVS sensors gives accurate frequency extraction from the fast-rotating objects within the scene \cite{hoseini2018real, rios2015real}. Implementation of real-time histogram creation on FPGAs is also significant in object tracking and optical flow calculation \cite{sethi2019optimized, pivezhandi2020ParaHist}.
